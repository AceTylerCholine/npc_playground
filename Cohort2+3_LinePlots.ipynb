{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7497da-51f7-4c39-a77b-8b9ecb5beb4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T17:55:45.474654Z",
     "iopub.status.busy": "2024-07-27T17:55:45.474654Z",
     "iopub.status.idle": "2024-07-27T17:55:53.160322Z",
     "shell.execute_reply": "2024-07-27T17:55:53.160322Z",
     "shell.execute_reply.started": "2024-07-27T17:55:45.474654Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeanal as spike\n",
    "\n",
    "cols = ['condition', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp', 'competition_closeness']\n",
    "\n",
    "comp_closeness_dict = {'Subj 1 blocking Subj 2': \"competitive\",\n",
    "'Subj 2 Only': \"no_comp\",\n",
    "'Subj 2 blocking Subj 1': \"competitive\",\n",
    "'Subj 1 then Subj 2': \"competitive\", \n",
    "'Subj 1 Only': \"no_comp\",\n",
    "'Subj 2 then Subj 1': \"competitive\",\n",
    "'Close Call': \"competitive\",\n",
    "'After trial': \"no_comp\"}\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('combined_excel_file.xlsx', usecols=cols, engine='openpyxl')\n",
    "\n",
    "df2 = df.dropna() # Drop the rows missing data\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) # Make the 'all_subjects' column readable as a list\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)] # Ignore novel sessions for now\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition']\n",
    "    compness = row['competition_closeness']\n",
    "    comp_label = comp_closeness_dict.get(compness, \"\")  # Default to \"\" if not found\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "\n",
    "        # Determine the new condition based on competition and win/lose logic\n",
    "        if condition in ['rewarded', 'omission', 'both_rewarded', 'tie']:\n",
    "            new_condition = condition\n",
    "        else:\n",
    "            win_or_lose = 'win' if str(condition) == str(subject) else 'lose'\n",
    "            new_condition = f\"{comp_label}_{win_or_lose}\"\n",
    "\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': new_condition,\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "valid_conditions = ['rewarded', 'omission', 'both_rewarded', 'tie', 'no_comp_win', 'no_comp_lose', 'competitive_win', 'competitive_lose']\n",
    "timestamp_dicts = {}\n",
    "\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in valid_conditions}\n",
    "    if condition in valid_conditions:\n",
    "        timestamp_dicts[key][condition].append(tuple_val)\n",
    "    else:\n",
    "        print(f\"Unexpected condition: {condition} for subj_recording {key}\")\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65e4c96-71f8-4d5d-ba9d-0b1041ab9c37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T17:55:54.587631Z",
     "iopub.status.busy": "2024-07-27T17:55:54.586623Z",
     "iopub.status.idle": "2024-07-27T17:58:14.111563Z",
     "shell.execute_reply": "2024-07-27T17:58:14.097562Z",
     "shell.execute_reply.started": "2024-07-27T17:55:54.587631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_112630_standard_comp_to_training_D1_subj_1-2_t2b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230613_105657_standard_comp_to_training_D2_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230613_105657_standard_comp_to_training_D2_subj_1-4_t4b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230614_114041_standard_comp_to_training_D3_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230614_114041_standard_comp_to_training_D3_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230616_111904_standard_comp_to_training_D4_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230616_111904_standard_comp_to_training_D4_subj_1-4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1-1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1-4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240320_142408_alone_comp_subj_3-1_t6b6_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240320_142408_alone_comp_subj_3-3_t5b5_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240320_171038_alone_comp_subj_4-2_t6b6_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240320_171038_alone_comp_subj_4-3_t5b5_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240322_120625_alone_comp_subj_3-3_t6b6_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240322_120625_alone_comp_subj_3-4_t5b5_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240322_160946_alone_comp_subj_4-3_t6b6_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240323_122227_alone_comp_subj_5-2_t6b6_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240323_144517_alone_comp_subj_3-1_t5b5_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240323_144517_alone_comp_subj_3-4_t6b6_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20240323_165815_alone_comp_subj_4-2_t5b5_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "# Construct the path in a platform-independent way (HiPerGator or Windows)\n",
    "ephys_path = Path('.') / 'recordings' / 'from_cyborg'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11caadea-7a30-4237-b23a-5237afc6e67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T17:58:14.150572Z",
     "iopub.status.busy": "2024-07-27T17:58:14.143564Z",
     "iopub.status.idle": "2024-07-27T17:58:23.831000Z",
     "shell.execute_reply": "2024-07-27T17:58:23.800451Z",
     "shell.execute_reply.started": "2024-07-27T17:58:14.150572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set to analyze\n"
     ]
    }
   ],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    start_pos = recording.find('subj_')\n",
    "    # Add the length of 'subj_' and 3 additional characters to include after 'subj_'\n",
    "    end_pos = start_pos + len('subj_') + 3\n",
    "    # Slice the recording key to get everything up to and including the subject identifier plus three characters\n",
    "    recording_key_without_suffix = recording[:end_pos]\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:start+3]\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject\n",
    "        \n",
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 250, ignore_freq = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae2c9ff-0eda-4d79-b027-e98b263c4d8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T17:58:23.899813Z",
     "iopub.status.busy": "2024-07-27T17:58:23.892809Z",
     "iopub.status.idle": "2024-07-27T17:58:32.947118Z",
     "shell.execute_reply": "2024-07-27T17:58:32.947118Z",
     "shell.execute_reply.started": "2024-07-27T17:58:23.898808Z"
    }
   },
   "outputs": [],
   "source": [
    "event_and_pre_event_spikes_df = spike_analysis.generate_event_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c81f4-05df-4d68-b52b-ee86d53a1ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
