{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1b795c-495e-49bb-8714-01a18850ac9b",
   "metadata": {},
   "source": [
    "# EPhys Data Analysis Tutorial\n",
    "\n",
    "This tutorial demonstrates a step-by-step approach to processing electrophysiology (EPhys) data for analysis using Meghan's multirecording_spikeanalysis script, the RCE Pilot 2 behavior spreadsheet, and directories of phy folders (with spike_times.npy, spike_clusters.npy, & cluster_group.tsv) for each ephys recording.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the libraries you'll be using, including Meghan's multirecording_spikeanalysis.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81770ad7-1919-4116-b994-5cdc0d77da9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:21:20.755836Z",
     "iopub.status.busy": "2024-05-08T15:21:20.755836Z",
     "iopub.status.idle": "2024-05-08T15:21:23.685427Z",
     "shell.execute_reply": "2024-05-08T15:21:23.685427Z",
     "shell.execute_reply.started": "2024-05-08T15:21:20.755836Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import multirecording_spikeanalysis as spike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695e1cb-5bf4-4fe2-979b-f1f2cb4fb677",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "First, we load the relevant EPhys data from the RCE Pilot 2 spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4875ae34-edf9-45e8-a7e0-49a1661231b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:21:25.604460Z",
     "iopub.status.busy": "2024-05-08T15:21:25.604460Z",
     "iopub.status.idle": "2024-05-08T15:21:26.875629Z",
     "shell.execute_reply": "2024-05-08T15:21:26.875629Z",
     "shell.execute_reply.started": "2024-05-08T15:21:25.604460Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aac8f4-0a3d-4b6c-8d6b-737ad89a1c5b",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Next, we rearrange the spreadsheet in order for it to prepare it for ephys recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc1b799-5a71-43ea-8f9e-f38cce47dd99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:21:28.412332Z",
     "iopub.status.busy": "2024-05-08T15:21:28.412332Z",
     "iopub.status.idle": "2024-05-08T15:21:28.465508Z",
     "shell.execute_reply": "2024-05-08T15:21:28.465508Z",
     "shell.execute_reply.started": "2024-05-08T15:21:28.412332Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df.dropna() # Drop the rows missing data\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) # Make the 'all_subjects' column readable as a list\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)] # Ignore novel sessions for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f0136-d2f4-42bc-bd55-a01367e569de",
   "metadata": {},
   "source": [
    "## Data Structuring\n",
    "We'll structure the data into a new DataFrame that aligns with our analysis goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112f9507-69df-4874-8db4-2dda8571c338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:21:30.295617Z",
     "iopub.status.busy": "2024-05-08T15:21:30.295617Z",
     "iopub.status.idle": "2024-05-08T15:21:30.409107Z",
     "shell.execute_reply": "2024-05-08T15:21:30.409107Z",
     "shell.execute_reply.started": "2024-05-08T15:21:30.295617Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe64e4-6bad-4d03-b01a-1471e5710258",
   "metadata": {},
   "source": [
    "## Timestamp Dictionary Preparation\n",
    "Prepare dictionaries of event timestamps to match with the ephys recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5284a66-5274-4f20-a0f2-82e64e122a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:21:31.959269Z",
     "iopub.status.busy": "2024-05-08T15:21:31.959269Z",
     "iopub.status.idle": "2024-05-08T15:21:32.088307Z",
     "shell.execute_reply": "2024-05-08T15:21:32.088307Z",
     "shell.execute_reply.started": "2024-05-08T15:21:31.959269Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537d178-124d-4cd2-921d-d6de5d3f254c",
   "metadata": {},
   "source": [
    "## EPhys Recording Collection\n",
    "Load EPhys recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b94ceca-39f0-4933-a018-a5858d86bf27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:21:35.147066Z",
     "iopub.status.busy": "2024-05-08T15:21:35.147066Z",
     "iopub.status.idle": "2024-05-08T15:22:27.646034Z",
     "shell.execute_reply": "2024-05-08T15:22:27.646034Z",
     "shell.execute_reply.started": "2024-05-08T15:21:35.147066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1-1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1-4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 96 is unsorted & has 5811 spikes\n",
      "Unit 96 will be deleted\n",
      "Unit 95 is unsorted & has 6458 spikes\n",
      "Unit 95 will be deleted\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "# Construct the path in a platform-independent way (HiPerGator or Windows)\n",
    "ephys_path = Path('.') / 'export' / 'updated_phys' / 'non-novel' / 'all_non_novel'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab04be-9f97-41d5-86b8-7f33318c00bf",
   "metadata": {},
   "source": [
    "## Assign Dictionaries to Collection\n",
    "Create dictionaries for each recording, and assign it and the subject number to the recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac0dc2f-c4d9-4601-8dec-cd65d6be3ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:22:27.743431Z",
     "iopub.status.busy": "2024-05-08T15:22:27.743431Z",
     "iopub.status.idle": "2024-05-08T15:22:27.756543Z",
     "shell.execute_reply": "2024-05-08T15:22:27.756543Z",
     "shell.execute_reply.started": "2024-05-08T15:22:27.743431Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    start_pos = recording.find('subj_')\n",
    "    # Add the length of 'subj_' and 3 additional characters to include after 'subj_'\n",
    "    end_pos = start_pos + len('subj_') + 3\n",
    "    # Slice the recording key to get everything up to and including the subject identifier plus three characters\n",
    "    recording_key_without_suffix = recording[:end_pos]\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:start+3]\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77bcfd30-8401-4ad7-94a5-3dc274ae736d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:30:10.420973Z",
     "iopub.status.busy": "2024-05-08T15:30:10.420973Z",
     "iopub.status.idle": "2024-05-08T15:31:19.908863Z",
     "shell.execute_reply": "2024-05-08T15:31:19.908863Z",
     "shell.execute_reply.started": "2024-05-08T15:30:10.420973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event1 vs event2\n",
       "not significant    249\n",
       "increases           32\n",
       "decreases            3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 5, smoothing_window=250, ignore_freq = 0.5)\n",
    "\n",
    "win_wilcox_df = spike_analysis.wilcox_baseline_v_event_collection('win', 10, 10, plot=False)\n",
    "\n",
    "win_wilcox_df[\"event1 vs event2\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbbe398-9413-4d10-be29-f234c4b599bd",
   "metadata": {},
   "source": [
    "### 12.3% cells significant for 5ms timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29cc6184-8f94-4e00-9768-88235f63d573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:29:30.018590Z",
     "iopub.status.busy": "2024-05-08T15:29:30.018590Z",
     "iopub.status.idle": "2024-05-08T15:30:03.706151Z",
     "shell.execute_reply": "2024-05-08T15:30:03.701137Z",
     "shell.execute_reply.started": "2024-05-08T15:29:30.018590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event1 vs event2\n",
       "not significant    245\n",
       "increases           37\n",
       "decreases            2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 10, smoothing_window=250, ignore_freq = 0.5)\n",
    "\n",
    "win_wilcox_df = spike_analysis.wilcox_baseline_v_event_collection('win', 10, 10, plot=False)\n",
    "\n",
    "win_wilcox_df[\"event1 vs event2\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb486de-bb5d-4090-b51a-ba6a9a66a8df",
   "metadata": {},
   "source": [
    "### 13.7% of cells significant for 10ms timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5b8b23a-4384-4160-af37-6703d53c03c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:31:44.683983Z",
     "iopub.status.busy": "2024-05-08T15:31:44.683983Z",
     "iopub.status.idle": "2024-05-08T15:31:50.854120Z",
     "shell.execute_reply": "2024-05-08T15:31:50.854120Z",
     "shell.execute_reply.started": "2024-05-08T15:31:44.683983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "C:\\Users\\short\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event1 vs event2\n",
       "not significant    240\n",
       "increases           38\n",
       "decreases            6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 100, smoothing_window=250, ignore_freq = 0.5)\n",
    "\n",
    "win_wilcox_df = spike_analysis.wilcox_baseline_v_event_collection('win', 10, 10, plot=False)\n",
    "\n",
    "win_wilcox_df[\"event1 vs event2\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767d1c8-229e-4e11-84c3-14acb025b039",
   "metadata": {},
   "source": [
    "### 15.5% of cells significant for 100ms timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bffcdfd1-9ca0-4a10-874b-8ec332bea5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:32:59.477122Z",
     "iopub.status.busy": "2024-05-08T15:32:59.477122Z",
     "iopub.status.idle": "2024-05-08T15:35:30.193791Z",
     "shell.execute_reply": "2024-05-08T15:35:30.187258Z",
     "shell.execute_reply.started": "2024-05-08T15:32:59.477122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set to analyze\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 790. MiB for an array with shape (26, 3984843) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spike_analysis \u001b[38;5;241m=\u001b[39m spike\u001b[38;5;241m.\u001b[39mSpikeAnalysis_MultiRecording(ephys_data, timebin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, smoothing_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, ignore_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      3\u001b[0m win_wilcox_df \u001b[38;5;241m=\u001b[39m spike_analysis\u001b[38;5;241m.\u001b[39mwilcox_baseline_v_event_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m win_wilcox_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent1 vs event2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\npc_playground\\multirecording_spikeanalysis.py:636\u001b[0m, in \u001b[0;36mSpikeAnalysis_MultiRecording.__init__\u001b[1;34m(self, ephyscollection, smoothing_window, timebin, ignore_freq)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_freq \u001b[38;5;241m=\u001b[39m ignore_freq\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPCA_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__all_set__()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\npc_playground\\multirecording_spikeanalysis.py:693\u001b[0m, in \u001b[0;36mSpikeAnalysis_MultiRecording.__all_set__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_whole_spiketrain__()\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_unit_spiketrains__()\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_unit_firing_rates__()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\npc_playground\\multirecording_spikeanalysis.py:778\u001b[0m, in \u001b[0;36mSpikeAnalysis_MultiRecording.__get_unit_firing_rates__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    772\u001b[0m     unit_firing_rates[unit] \u001b[38;5;241m=\u001b[39m get_firing_rate(\n\u001b[0;32m    773\u001b[0m         recording\u001b[38;5;241m.\u001b[39munit_spiketrains[unit],\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmoothing_window,\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimebin,\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m recording\u001b[38;5;241m.\u001b[39munit_firing_rates \u001b[38;5;241m=\u001b[39m unit_firing_rates\n\u001b[1;32m--> 778\u001b[0m recording\u001b[38;5;241m.\u001b[39munit_firing_rate_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    779\u001b[0m     [unit_firing_rates[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m unit_firing_rates]\n\u001b[0;32m    780\u001b[0m )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 790. MiB for an array with shape (26, 3984843) and data type float64"
     ]
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 1, smoothing_window=250, ignore_freq = 0.5)\n",
    "\n",
    "win_wilcox_df = spike_analysis.wilcox_baseline_v_event_collection('win', 10, 10, plot=False)\n",
    "\n",
    "win_wilcox_df[\"event1 vs event2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422ea9e-cc49-43c3-bbdb-49f89b8c7f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
