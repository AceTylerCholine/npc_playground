{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a403c67-21b6-406f-adcb-3a5f693e9b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T21:46:44.740607Z",
     "iopub.status.busy": "2024-05-31T21:46:44.740607Z",
     "iopub.status.idle": "2024-05-31T21:47:04.676093Z",
     "shell.execute_reply": "2024-05-31T21:47:04.674672Z",
     "shell.execute_reply.started": "2024-05-31T21:46:44.740607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import multirecording_spikeanalysis_edit as spike\n",
    "\n",
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')\n",
    "\n",
    "df2 = df.dropna() # Drop the rows missing data\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) # Make the 'all_subjects' column readable as a list\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)] # Ignore novel sessions for now\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)\n",
    "        \n",
    "\n",
    "# Construct the path in a platform-independent way (HiPerGator or Windows)\n",
    "ephys_path = Path('.') / 'export' / 'updated_phys' / 'test'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9353aa-4e3c-4115-b264-bdcbdd2f5714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T21:47:04.676795Z",
     "iopub.status.busy": "2024-05-31T21:47:04.676795Z",
     "iopub.status.idle": "2024-05-31T21:47:05.243912Z",
     "shell.execute_reply": "2024-05-31T21:47:05.243912Z",
     "shell.execute_reply.started": "2024-05-31T21:47:04.676795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set to analyze\n"
     ]
    }
   ],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    start_pos = recording.find('subj_')\n",
    "    # Add the length of 'subj_' and 3 additional characters to include after 'subj_'\n",
    "    end_pos = start_pos + len('subj_') + 3\n",
    "    # Slice the recording key to get everything up to and including the subject identifier plus three characters\n",
    "    recording_key_without_suffix = recording[:end_pos]\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:start+3]\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject\n",
    "        \n",
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 100, smoothing_window=250, ignore_freq = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173a47d6-bbf5-4c9d-bf75-2102af1e625e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T21:47:05.243912Z",
     "iopub.status.busy": "2024-05-31T21:47:05.243912Z",
     "iopub.status.idle": "2024-05-31T21:47:08.458959Z",
     "shell.execute_reply": "2024-05-31T21:47:08.456449Z",
     "shell.execute_reply.started": "2024-05-31T21:47:05.243912Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'event_times.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m unit_pvals\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m event_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_times.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Assuming event data is loaded\u001b[39;00m\n\u001b[0;32m     47\u001b[0m pvals \u001b[38;5;241m=\u001b[39m analyze_firing_rates(ephyscollection, event_data, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Print units with significant changes (adjust for significance level)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'event_times.npy'"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Poisson\n",
    "\n",
    "def analyze_firing_rates(ephyscollection, event, timebin, baseline_window, pre_window=0, post_window=0, equalize=0.5):\n",
    "  \"\"\"\n",
    "  Analyzes firing rate changes during events using Poisson GLM.\n",
    "\n",
    "  Args:\n",
    "      ephyscollection: An EphysRecordingCollection object.\n",
    "      event: The event name (str) or event data (numpy array).\n",
    "      timebin: Time bin size in seconds.\n",
    "      baseline_window: Baseline window size in seconds.\n",
    "      pre_window: Pre-event window size in seconds (default: 0).\n",
    "      post_window: Post-event window size in seconds (default: 0).\n",
    "      equalize: Time window size (in seconds) to equalize event durations (default: 0.5).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary containing unit IDs as keys and p-values as values.\n",
    "  \"\"\"\n",
    "\n",
    "  unit_pvals = {}\n",
    "  for recording_name, recording in ephyscollection.collection.items():\n",
    "    # Get unit spiketrains and baseline firing rates\n",
    "    recording.__get_unit_spiketrains__(timebin)\n",
    "    baseline_rates = recording.__calc_preevent_baseline__(baseline_window, baseline_window, 0, event)\n",
    "\n",
    "    # Prepare data for GLM\n",
    "    for unit, spiketrain in recording.unit_spiketrains.items():\n",
    "      if len(spiketrain) == 0:\n",
    "        continue\n",
    "      event_rates = recording.__get_unit_event_firing_rates__(unit, event, equalize, pre_window, post_window)\n",
    "      response = np.concatenate([*event_rates])\n",
    "      design = np.ones((len(response), 2))\n",
    "      design[:, 1] = np.concatenate([*baseline_rates[unit]])\n",
    "\n",
    "      # Fit Poisson GLM\n",
    "      glm_model = Poisson(response, design)\n",
    "      glm_result = glm_model.fit()\n",
    "\n",
    "      # Get p-value for baseline effect\n",
    "      unit_pvals[f\"{recording_name}-{unit}\"] = glm_result.pvalues[1]\n",
    "\n",
    "  return unit_pvals\n",
    "\n",
    "# Example usage\n",
    "event_data = np.load(\"event_times.npy\")  # Assuming event data is loaded\n",
    "pvals = analyze_firing_rates(ephyscollection, event_data, 0.001, 0.2)\n",
    "\n",
    "# Print units with significant changes (adjust for significance level)\n",
    "for unit, pval in pvals.items():\n",
    "  if pval < 0.05:\n",
    "    print(f\"Unit {unit} has significant change in firing rate during event (p-value: {pval})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91607a-3da0-4bf7-8266-9e73537650ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
