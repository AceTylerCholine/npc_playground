{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111bb370-12e1-4997-8d09-488f4b874d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T16:18:44.267478Z",
     "iopub.status.busy": "2024-05-17T16:18:44.251857Z",
     "iopub.status.idle": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply": "2024-05-17T16:18:48.110156Z",
     "shell.execute_reply.started": "2024-05-17T16:18:44.267478Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multirecording_spikeanalysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultirecording_spikeanalysis\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspike\u001b[39;00m\n\u001b[0;32m      9\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_dir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_subjects\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtone_start_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtone_stop_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'multirecording_spikeanalysis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import multirecording_spikeanalysis as spike\n",
    "\n",
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')\n",
    "\n",
    "df2 = df.dropna() # Drop the rows missing data\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) # Make the 'all_subjects' column readable as a list\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)] # Ignore novel sessions for now\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)\n",
    "        \n",
    "\n",
    "# Construct the path in a platform-independent way (HiPerGator or Windows)\n",
    "ephys_path = Path('.') / 'export' / 'updated_phys' / 'non-novel' / 'omission'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a533a64-f0a1-4033-9db4-5e3681546063",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-17T16:18:48.112676Z",
     "iopub.status.idle": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply.started": "2024-05-17T16:18:48.112676Z"
    }
   },
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    start_pos = recording.find('subj_')\n",
    "    # Add the length of 'subj_' and 3 additional characters to include after 'subj_'\n",
    "    end_pos = start_pos + len('subj_') + 3\n",
    "    # Slice the recording key to get everything up to and including the subject identifier plus three characters\n",
    "    recording_key_without_suffix = recording[:end_pos]\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:start+3]\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject\n",
    "        \n",
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data, timebin = 100, smoothing_window=250, ignore_freq = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099824b2-d0ba-4d84-a7eb-b6c38906bd73",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-17T16:18:48.112676Z",
     "iopub.status.idle": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply.started": "2024-05-17T16:18:48.112676Z"
    }
   },
   "outputs": [],
   "source": [
    "rewarded_df = spike_analysis.wilcox_baseline_v_event_collection('rewarded', 10, 10, plot=False)\n",
    "both_rewarded_df = spike_analysis.wilcox_baseline_v_event_collection('both_rewarded', 10, 10, plot=False)\n",
    "tie_df = spike_analysis.wilcox_baseline_v_event_collection('tie', 10, 10, plot=False)\n",
    "win_df = spike_analysis.wilcox_baseline_v_event_collection('win', 10, 10, plot=False)\n",
    "lose_df = spike_analysis.wilcox_baseline_v_event_collection('lose', 10, 10, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f1949-d6d9-4b8f-9ffe-91836a4b9c20",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-17T16:18:48.112676Z",
     "iopub.status.idle": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply": "2024-05-17T16:18:48.112676Z",
     "shell.execute_reply.started": "2024-05-17T16:18:48.112676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Concatenate the DataFrames\n",
    "combined_df = pd.concat([rewarded_df, both_rewarded_df, tie_df, win_df, lose_df])\n",
    "\n",
    "# Function to extract just the event name\n",
    "def extract_event_name(event):\n",
    "    # Splits the string and extracts the part before 'vs'\n",
    "    return event.split(' vs ')[0].replace('10s ', '')\n",
    "\n",
    "# Initialize an empty dictionary to hold aggregated data\n",
    "aggregated_data = {}\n",
    "\n",
    "# Iterate over the rows of the combined DataFrame\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Extract event name\n",
    "    event_name = extract_event_name(row['Event'])\n",
    "    # Construct a unique key for each combination of recording, subject, and unit id\n",
    "    key = (row['Recording'], row['Subject'], row['original unit id'])\n",
    "    \n",
    "    # Initialize the entry if not exists\n",
    "    if key not in aggregated_data:\n",
    "        aggregated_data[key] = {'sig_increase_events': set(), 'sig_decrease_events': set(), 'insig_events': set()}\n",
    "    \n",
    "    # Append the event name to the appropriate category based on 'event1 vs event2'\n",
    "    if row['event1 vs event2'] == 'increases':\n",
    "        aggregated_data[key]['sig_increase_events'].add(event_name)\n",
    "    elif row['event1 vs event2'] == 'decreases':\n",
    "        aggregated_data[key]['sig_decrease_events'].add(event_name)\n",
    "    else:\n",
    "        aggregated_data[key]['insig_events'].add(event_name)\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "rows = []\n",
    "for (recording, subject, unit_id), categories in aggregated_data.items():\n",
    "    row = {\n",
    "        'Recording': recording,\n",
    "        'Subject': subject,\n",
    "        'original unit id': unit_id,\n",
    "        'sig_increase_events': ', '.join(categories['sig_increase_events']),\n",
    "        'sig_decrease_events': ', '.join(categories['sig_decrease_events']),\n",
    "        'insig_events': ', '.join(categories['insig_events']),\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "aggregated_df = pd.DataFrame(rows)\n",
    "\n",
    "aggregated_df['sig_events'] = aggregated_df.apply(lambda row: ', '.join(filter(None, [row['sig_increase_events'], row['sig_decrease_events']])), axis=1)\n",
    "\n",
    "# Function to normalize event strings\n",
    "def normalize_event_string(event_string):\n",
    "    event_list = event_string.split(', ')\n",
    "    event_list_sorted = sorted(event_list)  # Sort the list to ensure consistent order\n",
    "    return ', '.join(event_list_sorted)  # Join the sorted list back into a string\n",
    "\n",
    "# Apply the normalization function to the 'sig_events' column before counting\n",
    "aggregated_df['sig_events'] = aggregated_df['sig_events'].apply(normalize_event_string)\n",
    "\n",
    "# Count the unique combinations of significant events\n",
    "sig_events_counts = aggregated_df['sig_events'].value_counts()\n",
    "\n",
    "# Rename the empty strings to 'non-specific'\n",
    "sig_events_counts = sig_events_counts.rename(index={'': 'non-responsive'})\n",
    "\n",
    "# Group indexes containing ',' into 'multi-event specific'\n",
    "# sig_events_counts['multi-event'] = sig_events_counts[[',' in index for index in sig_events_counts.index]].sum()\n",
    "# sig_events_counts = sig_events_counts.drop(sig_events_counts[[',' in index for index in sig_events_counts.index]].index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(sig_events_counts, labels=sig_events_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Omission: Uniquely Responsive Neurons')\n",
    "plt.show()\n",
    "# plt.savefig('Omission_Wilcoxon_Pieplot_100msTimebin.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11516f1-7d1f-43ac-989b-323e79a7a3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
