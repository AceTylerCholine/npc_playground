{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f75cd85-3cbc-4a97-929a-f0b9125e75fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied phy files for '20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec' to the updated directory.\n",
      "Copied phy files for '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs1-2and2-2_merged.rec' to the updated directory.\n",
      "Not all required files are present in .\\export\\finished_curation\\20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs1-2and2-2_merged.rec\\phy\\.phy\\phy. Skipping...\n",
      "Copied phy files for '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-2vs1-1and2-1_merged.rec' to the updated directory.\n",
      "Not all required files are present in .\\export\\finished_curation\\20230628_111202_standard_comp_to_novel_agent_D1_subj_1-2vs1-1and2-1_merged.rec\\phy\\.phy\\phy. Skipping...\n",
      "Copied phy files for '20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1v1-4and2-1_merged.rec' to the updated directory.\n",
      "Not all required files are present in .\\export\\finished_curation\\20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1v1-4and2-1_merged.rec\\phy\\.phy\\phy. Skipping...\n",
      "Copied phy files for '20230629_111937_standard_comp_to_novel_agent_D2_subj_1-4vs1-1and2-2_merged.rec' to the updated directory.\n",
      "Not all required files are present in .\\export\\finished_curation\\20230629_111937_standard_comp_to_novel_agent_D2_subj_1-4vs1-1and2-2_merged.rec\\phy\\.phy\\phy. Skipping...\n",
      "Copied phy files for '20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4vs1-2and2-1_merged_merged.rec' to the updated directory.\n",
      "Not all required files are present in .\\export\\finished_curation\\20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4vs1-2and2-1_merged_merged.rec\\phy\\.phy\\phy. Skipping...\n",
      "Finished copying phy files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the source and destination root directories\n",
    "source_root = '.\\\\export\\\\finished_curation'\n",
    "destination_root = '.\\\\export\\\\updated_phys'\n",
    "\n",
    "# Walk through the directory\n",
    "for dirpath, dirnames, files in os.walk(source_root):\n",
    "    # Check if 'phy' is in the current directory path\n",
    "    if 'phy' in dirnames:\n",
    "        # Construct the path to the 'phy' folder\n",
    "        phy_path = os.path.join(dirpath, 'phy')\n",
    "        \n",
    "        # Check for the presence of the required files\n",
    "        required_files = ['cluster_group.tsv', 'spike_clusters.npy', 'spike_times.npy']\n",
    "        existing_files = [f for f in required_files if f in os.listdir(phy_path)]\n",
    "        \n",
    "        # If all required files are present, proceed with copying\n",
    "        if len(existing_files) == len(required_files):\n",
    "            # Extract the 'name' part of the path (assuming it's the folder right before 'phy')\n",
    "            name = os.path.basename(os.path.dirname(phy_path))\n",
    "            \n",
    "            # Construct the new directory path\n",
    "            new_dir = os.path.join(destination_root, name, 'phy')\n",
    "            os.makedirs(new_dir, exist_ok=True)\n",
    "            \n",
    "            # Copy each file\n",
    "            for file_name in required_files:\n",
    "                src_file_path = os.path.join(phy_path, file_name)\n",
    "                dest_file_path = os.path.join(new_dir, file_name)\n",
    "                shutil.copy2(src_file_path, dest_file_path)\n",
    "            print(f\"Copied phy files for '{name}' to the updated directory.\")\n",
    "        else:\n",
    "            print(f\"Not all required files are present in {phy_path}. Skipping...\")\n",
    "\n",
    "print(\"Finished copying phy files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431d0e1e-0d0b-44b9-8a59-7bb535342422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')\n",
    "df2 = df.dropna()\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df3.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47f96af-e5ae-471c-aa05-692437d5a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 96 is unsorted & has 5811 spikes\n",
      "Unit 96 will be deleted\n",
      "Unit 95 is unsorted & has 6458 spikes\n",
      "Unit 95 will be deleted\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs1-2and2-2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230628_111202_standard_comp_to_novel_agent_D1_subj_1-2vs1-1and2-1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1v1-4and2-1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230629_111937_standard_comp_to_novel_agent_D2_subj_1-4vs1-1and2-2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4vs1-2and2-1_merged_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import multirecording_spikeanalysis as spike\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Construct the path in a platform-independent way\n",
    "path = Path('.') / 'export' / 'updated_phys'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfe2031-d282-47c0-b0a8-a9492060685c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20230612_101430_standard_comp_to_training_D1_subj_1-3',\n",
       " '20230612_101430_standard_comp_to_training_D1_subj_1-4',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-1',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-2',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-1',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-4',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-1',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-4',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-1',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-2',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-1',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-4',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-1',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-4',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4',\n",
       " '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1',\n",
       " '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-2',\n",
       " '20230628_111202_standard_comp_to_novel_agent_D1_subj_2-1',\n",
       " '20230628_111202_standard_comp_to_novel_agent_D1_subj_2-2',\n",
       " '20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1',\n",
       " '20230629_111937_standard_comp_to_novel_agent_D2_subj_1-4',\n",
       " '20230629_111937_standard_comp_to_novel_agent_D2_subj_2-1',\n",
       " '20230629_111937_standard_comp_to_novel_agent_D2_subj_2-2',\n",
       " '20230630_115506_standard_comp_to_novel_agent_D3_subj_1-2',\n",
       " '20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4',\n",
       " '20230630_115506_standard_comp_to_novel_agent_D3_subj_2-1',\n",
       " '20230630_115506_standard_comp_to_novel_agent_D3_subj_2-2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(timestamp_dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5f8f4c-67be-4a36-a085-e6f1b1aa5c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_dir</th>\n",
       "      <th>subject</th>\n",
       "      <th>subj_recording</th>\n",
       "      <th>condition</th>\n",
       "      <th>tone_start_timestamp</th>\n",
       "      <th>tone_stop_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>lose</td>\n",
       "      <td>982229.0</td>\n",
       "      <td>1182226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>win</td>\n",
       "      <td>982229.0</td>\n",
       "      <td>1182226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>win</td>\n",
       "      <td>3382227.0</td>\n",
       "      <td>3582224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>lose</td>\n",
       "      <td>3382227.0</td>\n",
       "      <td>3582224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>lose</td>\n",
       "      <td>5682225.0</td>\n",
       "      <td>5882222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>win</td>\n",
       "      <td>64173502.0</td>\n",
       "      <td>64373501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>lose</td>\n",
       "      <td>66073522.0</td>\n",
       "      <td>66273524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>win</td>\n",
       "      <td>66073522.0</td>\n",
       "      <td>66273524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>lose</td>\n",
       "      <td>67073535.0</td>\n",
       "      <td>67273537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>win</td>\n",
       "      <td>67073535.0</td>\n",
       "      <td>67273537.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1630 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            session_dir subject  \\\n",
       "0     20230612_101430_standard_comp_to_training_D1_s...     1.3   \n",
       "1     20230612_101430_standard_comp_to_training_D1_s...     1.4   \n",
       "2     20230612_101430_standard_comp_to_training_D1_s...     1.3   \n",
       "3     20230612_101430_standard_comp_to_training_D1_s...     1.4   \n",
       "4     20230612_101430_standard_comp_to_training_D1_s...     1.3   \n",
       "...                                                 ...     ...   \n",
       "2161  20230630_115506_standard_comp_to_novel_agent_D...     2.2   \n",
       "2164  20230630_115506_standard_comp_to_novel_agent_D...     2.1   \n",
       "2165  20230630_115506_standard_comp_to_novel_agent_D...     2.2   \n",
       "2168  20230630_115506_standard_comp_to_novel_agent_D...     2.1   \n",
       "2169  20230630_115506_standard_comp_to_novel_agent_D...     2.2   \n",
       "\n",
       "                                         subj_recording condition  \\\n",
       "0     20230612_101430_standard_comp_to_training_D1_s...      lose   \n",
       "1     20230612_101430_standard_comp_to_training_D1_s...       win   \n",
       "2     20230612_101430_standard_comp_to_training_D1_s...       win   \n",
       "3     20230612_101430_standard_comp_to_training_D1_s...      lose   \n",
       "4     20230612_101430_standard_comp_to_training_D1_s...      lose   \n",
       "...                                                 ...       ...   \n",
       "2161  20230630_115506_standard_comp_to_novel_agent_D...       win   \n",
       "2164  20230630_115506_standard_comp_to_novel_agent_D...      lose   \n",
       "2165  20230630_115506_standard_comp_to_novel_agent_D...       win   \n",
       "2168  20230630_115506_standard_comp_to_novel_agent_D...      lose   \n",
       "2169  20230630_115506_standard_comp_to_novel_agent_D...       win   \n",
       "\n",
       "      tone_start_timestamp  tone_stop_timestamp  \n",
       "0                 982229.0            1182226.0  \n",
       "1                 982229.0            1182226.0  \n",
       "2                3382227.0            3582224.0  \n",
       "3                3382227.0            3582224.0  \n",
       "4                5682225.0            5882222.0  \n",
       "...                    ...                  ...  \n",
       "2161            64173502.0           64373501.0  \n",
       "2164            66073522.0           66273524.0  \n",
       "2165            66073522.0           66273524.0  \n",
       "2168            67073535.0           67273537.0  \n",
       "2169            67073535.0           67273537.0  \n",
       "\n",
       "[1630 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3522783-ad85-4d89-94dc-44274b657087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>session_dir</th>\n",
       "      <th>all_subjects</th>\n",
       "      <th>tone_start_timestamp</th>\n",
       "      <th>tone_stop_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>982229.0</td>\n",
       "      <td>1182226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>3382227.0</td>\n",
       "      <td>3582224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>5682225.0</td>\n",
       "      <td>5882222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>7482224.0</td>\n",
       "      <td>7682221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>60673457.0</td>\n",
       "      <td>60873459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>62173475.0</td>\n",
       "      <td>62373477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>64173502.0</td>\n",
       "      <td>64373501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>66073522.0</td>\n",
       "      <td>66273524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>67073535.0</td>\n",
       "      <td>67273537.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     condition                                         session_dir  \\\n",
       "0           NaN                                                NaN   \n",
       "1           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "2           1.3  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "3           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "4           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "...         ...                                                ...   \n",
       "1219        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1220        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1221        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1222        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1223        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "\n",
       "                      all_subjects  tone_start_timestamp  tone_stop_timestamp  \n",
       "0                              NaN                   NaN                  NaN  \n",
       "1                   ['1.3', '1.4']              982229.0            1182226.0  \n",
       "2                   ['1.3', '1.4']             3382227.0            3582224.0  \n",
       "3                   ['1.3', '1.4']             5682225.0            5882222.0  \n",
       "4                   ['1.3', '1.4']             7482224.0            7682221.0  \n",
       "...                            ...                   ...                  ...  \n",
       "1219  ['1.2', '1.4', '2.1', '2.2']            60673457.0           60873459.0  \n",
       "1220  ['1.2', '1.4', '2.1', '2.2']            62173475.0           62373477.0  \n",
       "1221  ['1.2', '1.4', '2.1', '2.2']            64173502.0           64373501.0  \n",
       "1222  ['1.2', '1.4', '2.1', '2.2']            66073522.0           66273524.0  \n",
       "1223  ['1.2', '1.4', '2.1', '2.2']            67073535.0           67273537.0  \n",
       "\n",
       "[1224 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1851b569-659b-4921-b7a4-6880d702fa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>session_dir</th>\n",
       "      <th>all_subjects</th>\n",
       "      <th>tone_start_timestamp</th>\n",
       "      <th>tone_stop_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>[1.3, 1.4]</td>\n",
       "      <td>982229.0</td>\n",
       "      <td>1182226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>[1.3, 1.4]</td>\n",
       "      <td>3382227.0</td>\n",
       "      <td>3582224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>[1.3, 1.4]</td>\n",
       "      <td>5682225.0</td>\n",
       "      <td>5882222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>[1.3, 1.4]</td>\n",
       "      <td>7482224.0</td>\n",
       "      <td>7682221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>[1.3, 1.4]</td>\n",
       "      <td>8582220.0</td>\n",
       "      <td>8782223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>[1.2, 1.4, 2.1, 2.2]</td>\n",
       "      <td>60673457.0</td>\n",
       "      <td>60873459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>[1.2, 1.4, 2.1, 2.2]</td>\n",
       "      <td>62173475.0</td>\n",
       "      <td>62373477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>[1.2, 1.4, 2.1, 2.2]</td>\n",
       "      <td>64173502.0</td>\n",
       "      <td>64373501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>[1.2, 1.4, 2.1, 2.2]</td>\n",
       "      <td>66073522.0</td>\n",
       "      <td>66273524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>2.2</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>[1.2, 1.4, 2.1, 2.2]</td>\n",
       "      <td>67073535.0</td>\n",
       "      <td>67273537.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>848 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     condition                                         session_dir  \\\n",
       "1           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "2           1.3  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "3           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "4           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "5           1.4  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "...         ...                                                ...   \n",
       "1219        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1220        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1221        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1222        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1223        2.2  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "\n",
       "              all_subjects  tone_start_timestamp  tone_stop_timestamp  \n",
       "1               [1.3, 1.4]              982229.0            1182226.0  \n",
       "2               [1.3, 1.4]             3382227.0            3582224.0  \n",
       "3               [1.3, 1.4]             5682225.0            5882222.0  \n",
       "4               [1.3, 1.4]             7482224.0            7682221.0  \n",
       "5               [1.3, 1.4]             8582220.0            8782223.0  \n",
       "...                    ...                   ...                  ...  \n",
       "1219  [1.2, 1.4, 2.1, 2.2]            60673457.0           60873459.0  \n",
       "1220  [1.2, 1.4, 2.1, 2.2]            62173475.0           62373477.0  \n",
       "1221  [1.2, 1.4, 2.1, 2.2]            64173502.0           64373501.0  \n",
       "1222  [1.2, 1.4, 2.1, 2.2]            66073522.0           66273524.0  \n",
       "1223  [1.2, 1.4, 2.1, 2.2]            67073535.0           67273537.0  \n",
       "\n",
       "[848 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd0694b-a28e-4c93-bded-6b443b8a46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376a8697-4c54-419a-8ae7-f5f37148409a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df4\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df4' is not defined"
     ]
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca73f6be-249f-4a03-88a3-30338572e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 96 is unsorted & has 5811 spikes\n",
      "Unit 96 will be deleted\n",
      "Unit 95 is unsorted & has 6458 spikes\n",
      "Unit 95 will be deleted\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "import multirecording_spikeanalysis as spike\n",
    "from pathlib import Path\n",
    "\n",
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')\n",
    "df2 = df.dropna()\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)]\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)\n",
    "\n",
    "\n",
    "# Construct the path in a platform-independent way\n",
    "ephys_path = Path('.') / 'export' / 'updated_phys' / 'non-novel'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef73552-c397-4eaf-b2be-a2516bd4abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    recording_key_without_suffix = recording[:-22]  # Remove everything after subject # from the end\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:].replace('-', '_')  # Replace '-' with '_' (I've forgotten why I do this step. Does subject need to be in a specific format?)\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f53f308-fdca-405a-96c0-9d9d675962cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These recordings are missing event dictionaries:\n",
      "['20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n",
      "These recordings are missing subjects: ['20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n"
     ]
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2945ea-7057-404e-b3d7-ac90f202cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20230612_101430_standard_comp_to_training_D1_subj_1-3',\n",
       " '20230612_101430_standard_comp_to_training_D1_subj_1-4',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-1',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-2',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-1',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-4',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-1',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-4',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-1',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-2',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-1',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-4',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-1',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-4',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(timestamp_dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631bb769-82d5-458b-b009-d6a11a3f6e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (4101953843.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    timestamp_dicts.20230618_100636_standard_comp_to_omission_D2_subj_1-1\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "timestamp_dicts.20230618_100636_standard_comp_to_omission_D2_subj_1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0e6041-9821-4db3-a84a-8ec3ac860489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewarded': array([[1854961, 1864961],\n",
       "        [1914961, 1924961],\n",
       "        [1969961, 1979961],\n",
       "        [2034961, 2044961],\n",
       "        [2089961, 2099961],\n",
       "        [2139961, 2149961],\n",
       "        [2189961, 2199961],\n",
       "        [2414961, 2424961],\n",
       "        [2534961, 2544961],\n",
       "        [2644961, 2654961],\n",
       "        [2729961, 2739961],\n",
       "        [2849961, 2859961],\n",
       "        [2974961, 2984961],\n",
       "        [3034961, 3044961],\n",
       "        [3109961, 3119961]], dtype=int64),\n",
       " 'win': array([[ 174962,  184962],\n",
       "        [ 289962,  299962],\n",
       "        [ 434962,  444962],\n",
       "        [ 759962,  769962],\n",
       "        [ 809962,  819962],\n",
       "        [ 889962,  899962],\n",
       "        [ 954962,  964962],\n",
       "        [1019962, 1029962],\n",
       "        [1069962, 1079962],\n",
       "        [1139962, 1149962],\n",
       "        [1234962, 1244962],\n",
       "        [1314962, 1324962],\n",
       "        [1384962, 1394961],\n",
       "        [1494962, 1504961]], dtype=int64),\n",
       " 'lose': array([[  54962,   64962],\n",
       "        [ 379962,  389962],\n",
       "        [ 484962,  494962],\n",
       "        [ 579962,  589962],\n",
       "        [ 654962,  664962],\n",
       "        [1554961, 1564961]], dtype=int64),\n",
       " 'omission': array([[2294961, 2304961],\n",
       "        [2909961, 2919961],\n",
       "        [3209961, 3219961]], dtype=int64),\n",
       " 'both_rewarded': array([], dtype=int64),\n",
       " 'tie': array([], dtype=int64)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_dicts['20230618_100636_standard_comp_to_omission_D2_subj_1-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1296c1df-f2f0-4289-8228-0239c872d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    recording_key_without_suffix = recording[:-22]  # Remove everything after subject # from the end\n",
    "    recording_key_without_suffix = recording_key_without_suffix.replace('-', '_')\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fb1c8fb-8ac9-466b-a135-bd87d75f8afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These recordings are missing event dictionaries:\n",
      "['20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n",
      "These recordings are missing subjects: ['20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n"
     ]
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d56493b-0595-44be-b209-bb538d5f9b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec', '20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec', '20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ephys_data.collection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc59e6a-9cfe-4d50-b2d8-1a82ab53203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec',\n",
       " '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ephys_data.collection.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ec09d4-fca1-47c1-a86a-620e3650e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F9415C150>\n",
      "----------\n",
      "Key: 20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F93EAB550>\n",
      "----------\n",
      "Key: 20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F93ECC210>\n",
      "----------\n",
      "Key: 20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F943A7910>\n",
      "----------\n",
      "Key: 20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F8CB82610>\n",
      "----------\n",
      "Key: 20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F943A78D0>\n",
      "----------\n",
      "Key: 20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F94336450>\n",
      "----------\n",
      "Key: 20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F94258CD0>\n",
      "----------\n",
      "Key: 20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F94337190>\n",
      "----------\n",
      "Key: 20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F8CBC8310>\n",
      "----------\n",
      "Key: 20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F93B73B90>\n",
      "----------\n",
      "Key: 20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F92D37DD0>\n",
      "----------\n",
      "Key: 20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F94336C50>\n",
      "----------\n",
      "Key: 20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F93EC6710>\n",
      "----------\n",
      "Key: 20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "Value: <multirecording_spikeanalysis.EphysRecording object at 0x0000015F93F7F3D0>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for key in ephys_data.collection.keys():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Value:\", ephys_data.collection[key])\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9e97af1-dc4c-4b66-b742-108c2d44b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EphysRecording' object has no attribute 'spike_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, recording \u001b[38;5;129;01min\u001b[39;00m ephys_data\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpike Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recording\u001b[38;5;241m.\u001b[39mspike_data)  \u001b[38;5;66;03m# Hypothetical attribute\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recording\u001b[38;5;241m.\u001b[39mrecording_name)  \u001b[38;5;66;03m# Hypothetical attribute\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EphysRecording' object has no attribute 'spike_data'"
     ]
    }
   ],
   "source": [
    "for key, recording in ephys_data.collection.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Spike Data:\", recording.spike_data)  # Hypothetical attribute\n",
    "    print(\"Recording Name:\", recording.recording_name)  # Hypothetical attribute\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b5ebafc-4db4-4c64-8c4a-ace74510fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EphysRecording' object has no attribute 'get_spike_rates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, recording \u001b[38;5;129;01min\u001b[39;00m ephys_data\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpike Rates:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recording\u001b[38;5;241m.\u001b[39mget_spike_rates())  \u001b[38;5;66;03m# Hypothetical method\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recording\u001b[38;5;241m.\u001b[39mcalculate_statistics())  \u001b[38;5;66;03m# Hypothetical method\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EphysRecording' object has no attribute 'get_spike_rates'"
     ]
    }
   ],
   "source": [
    "for key, recording in ephys_data.collection.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Spike Rates:\", recording.get_spike_rates())  # Hypothetical method\n",
    "    print(\"Statistics:\", recording.calculate_statistics())  # Hypothetical method\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be660b52-5ede-4bc6-ad12-9d032e949f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EphysRecording' object has no attribute 'spiketrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m labels_dict \u001b[38;5;241m=\u001b[39m specific_recording\u001b[38;5;241m.\u001b[39mlabels_dict\n\u001b[0;32m     12\u001b[0m unit_timestamps \u001b[38;5;241m=\u001b[39m specific_recording\u001b[38;5;241m.\u001b[39munit_timestamps\n\u001b[1;32m---> 13\u001b[0m spiketrain \u001b[38;5;241m=\u001b[39m specific_recording\u001b[38;5;241m.\u001b[39mspiketrain\n\u001b[0;32m     14\u001b[0m unit_spiketrains \u001b[38;5;241m=\u001b[39m specific_recording\u001b[38;5;241m.\u001b[39munit_spiketrains\n\u001b[0;32m     15\u001b[0m unit_firing_rates \u001b[38;5;241m=\u001b[39m specific_recording\u001b[38;5;241m.\u001b[39munit_firing_rates\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EphysRecording' object has no attribute 'spiketrain'"
     ]
    }
   ],
   "source": [
    "# Assuming you have an instance of EphysRecordingCollection called ephys_data\n",
    "recording_key = '20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec'\n",
    "specific_recording = ephys_data.collection[recording_key]\n",
    "\n",
    "# Now you can access the attributes of the EphysRecording object\n",
    "path = specific_recording.path\n",
    "subject = specific_recording.subject\n",
    "sampling_rate = specific_recording.sampling_rate\n",
    "timestamps_var = specific_recording.timestamps_var\n",
    "unit_array = specific_recording.unit_array\n",
    "labels_dict = specific_recording.labels_dict\n",
    "unit_timestamps = specific_recording.unit_timestamps\n",
    "spiketrain = specific_recording.spiketrain\n",
    "unit_spiketrains = specific_recording.unit_spiketrains\n",
    "unit_firing_rates = specific_recording.unit_firing_rates\n",
    "\n",
    "# For example, to print some of the attributes:\n",
    "print(f\"Path: {path}\")\n",
    "print(f\"Subject: {subject}\")\n",
    "print(f\"Sampling Rate: {sampling_rate}\")\n",
    "print(f\"Timestamps: {timestamps_var}\")\n",
    "print(f\"Unit Array: {unit_array}\")\n",
    "print(f\"Labels Dictionary: {labels_dict}\")\n",
    "print(f\"Unit Timestamps: {unit_timestamps}\")\n",
    "print(f\"Spiketrain: {spiketrain}\")\n",
    "print(f\"Unit Spiketrains: {unit_spiketrains}\")\n",
    "print(f\"Unit Firing Rates: {unit_firing_rates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3674b44a-4ccd-43eb-af4a-a407e5a901cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 96 is unsorted & has 5811 spikes\n",
      "Unit 96 will be deleted\n",
      "Unit 95 is unsorted & has 6458 spikes\n",
      "Unit 95 will be deleted\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "import multirecording_spikeanalysis as spike\n",
    "from pathlib import Path\n",
    "\n",
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')\n",
    "df2 = df.dropna()\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)]\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)\n",
    "\n",
    "\n",
    "# Construct the path in a platform-independent way\n",
    "ephys_path = Path('.') / 'export' / 'updated_phys' / 'non-novel'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4637577-d58b-4354-814e-565122b9a08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20230612_101430_standard_comp_to_training_D1_subj_1-3', '20230612_101430_standard_comp_to_training_D1_subj_1-4', '20230612_112630_standard_comp_to_training_D1_subj_1-1', '20230612_112630_standard_comp_to_training_D1_subj_1-2', '20230613_105657_standard_comp_to_training_D2_subj_1-1', '20230613_105657_standard_comp_to_training_D2_subj_1-4', '20230614_114041_standard_comp_to_training_D3_subj_1-1', '20230614_114041_standard_comp_to_training_D3_subj_1-2', '20230616_111904_standard_comp_to_training_D4_subj_1-2', '20230616_111904_standard_comp_to_training_D4_subj_1-4', '20230617_115521_standard_comp_to_omission_D1_subj_1-1', '20230617_115521_standard_comp_to_omission_D1_subj_1-2', '20230618_100636_standard_comp_to_omission_D2_subj_1-1', '20230618_100636_standard_comp_to_omission_D2_subj_1-4', '20230620_114347_standard_comp_to_omission_D4_subj_1-1', '20230620_114347_standard_comp_to_omission_D4_subj_1-2', '20230621_111240_standard_comp_to_omission_D5_subj_1-2', '20230621_111240_standard_comp_to_omission_D5_subj_1-4', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_dicts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a5b484-7aab-40d8-940b-d9e5c66670ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    recording_key_without_suffix = recording[:-22]  # Remove everything after subject # from the end\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d194489a-29c9-4c25-9247-1320505dcc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These recordings are missing event dictionaries:\n",
      "['20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n",
      "These recordings are missing subjects: ['20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n"
     ]
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4289f4c3-8886-4a77-903e-c5b3d04badc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1-1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1-4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 96 is unsorted & has 5811 spikes\n",
      "Unit 96 will be deleted\n",
      "Unit 95 is unsorted & has 6458 spikes\n",
      "Unit 95 will be deleted\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "import multirecording_spikeanalysis as spike\n",
    "from pathlib import Path\n",
    "\n",
    "cols = ['condition ', 'session_dir', 'all_subjects', 'tone_start_timestamp', 'tone_stop_timestamp']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx', usecols=cols, engine='openpyxl')\n",
    "df2 = df.dropna()\n",
    "df3 = df2.copy()\n",
    "df3['all_subjects'] = df3['all_subjects'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df4 = df3[df3['all_subjects'].apply(lambda x: len(x) < 3)]\n",
    "\n",
    "# Initialize an empty list to collect data for the new DataFrame\n",
    "new_df_data = []\n",
    "\n",
    "for _, row in df4.iterrows():\n",
    "    session_dir = row['session_dir']\n",
    "    subjects = row['all_subjects']\n",
    "    condition = row['condition ']\n",
    "\n",
    "    # Split session_dir on '_subj_' and take the first part only\n",
    "    # This ensures everything after '_subj_' is ignored\n",
    "    base_session_dir = session_dir.split('_subj_')[0]\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_formatted = subject.replace('.', '-')\n",
    "        # Append formatted subject to the base session_dir correctly\n",
    "        subj_recording = f\"{base_session_dir}_subj_{subject_formatted}\"\n",
    "        new_df_data.append({\n",
    "            'session_dir': session_dir,\n",
    "            'subject': subject,\n",
    "            'subj_recording': subj_recording,\n",
    "            'condition': condition if condition in ['rewarded', 'omission', 'both_rewarded', 'tie'] else ('win' if str(condition) == str(subject) else 'lose'),\n",
    "            'tone_start_timestamp': row['tone_start_timestamp'],\n",
    "            'tone_stop_timestamp': row['tone_stop_timestamp']\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert list to DataFrame\n",
    "new_df = pd.DataFrame(new_df_data)\n",
    "new_df = new_df.drop_duplicates()\n",
    "\n",
    "# Prepare timestamp_dicts from new_df\n",
    "timestamp_dicts = {}\n",
    "for _, row in new_df.iterrows():\n",
    "    key = row['subj_recording']\n",
    "    condition = row['condition']\n",
    "    timestamp_start = int(row['tone_start_timestamp']) // 20\n",
    "    timestamp_end = int(row['tone_stop_timestamp']) // 20\n",
    "    tuple_val = (timestamp_start, timestamp_end)\n",
    "\n",
    "    if key not in timestamp_dicts:\n",
    "        timestamp_dicts[key] = {cond: [] for cond in ['rewarded', 'win', 'lose', 'omission', 'both_rewarded', 'tie']}\n",
    "    timestamp_dicts[key][condition].append(tuple_val)\n",
    "\n",
    "# Convert lists in timestamp_dicts to numpy arrays\n",
    "for subj_recording in timestamp_dicts:\n",
    "    for condition in timestamp_dicts[subj_recording]:\n",
    "        timestamp_dicts[subj_recording][condition] = np.array(timestamp_dicts[subj_recording][condition], dtype=np.int64)\n",
    "\n",
    "\n",
    "# Construct the path in a platform-independent way\n",
    "ephys_path = Path('.') / 'export' / 'updated_phys' / 'non-novel'\n",
    "\n",
    "ephys_data = spike.EphysRecordingCollection(str(ephys_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7857017b-c28b-4178-aa9b-9e9c755181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    recording_key_without_suffix = recording[:-22]  # Remove everything after subject # from the end\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:]\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcb8f79-b0b8-46cb-9bcd-5d23076266df",
   "metadata": {},
   "outputs": [],
   "source": [
    "str123 = '1234567890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7105862e-1d49-4e3a-8759-9fb482aee938",
   "metadata": {},
   "outputs": [],
   "source": [
    "str0 = str123.find('789') + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94227cf3-32bb-48cd-9c05-dd5f65801985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5c4d4c-70f1-4a96-a935-b13b3039eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "str02 = str123[str0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0915693-661c-4aa8-b99b-b9d440d6439a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d183bf3b-fdc7-4634-937f-961c35c3d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These recordings are missing event dictionaries:\n",
      "['20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n",
      "These recordings are missing subjects: ['20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec']\n"
     ]
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3028934f-fb28-4425-a92b-25949545512b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20230612_101430_standard_comp_to_training_D1_subj_1-3',\n",
       " '20230612_101430_standard_comp_to_training_D1_subj_1-4',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-1',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-2',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-1',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-4',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-1',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-4',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-1',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-2',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-1',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-4',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-1',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-4',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(timestamp_dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5a13c8-29d5-4f7d-886f-0e8b61cf5d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4\n"
     ]
    }
   ],
   "source": [
    "test1 = '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec'\n",
    "test2 = '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec'\n",
    "testboth = [test1, test2]\n",
    "for test in testboth:\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    start_pos = recording.find('subj_')\n",
    "    # Add the length of 'subj_' and 3 additional characters to include after 'subj_'\n",
    "    end_pos = start_pos + len('subj_') + 3\n",
    "    # Slice the recording key to get everything up to and including the subject identifier plus three characters\n",
    "    recording_key_without_suffix = recording[:end_pos]\n",
    "    print (recording_key_without_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cbb4938-6a6b-44d5-821a-a1504eef7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in ephys_data.collection.keys():\n",
    "    # Check if the recording key (without everything after subject #) is in timestamp_dicts\n",
    "    start_pos = recording.find('subj_')\n",
    "    # Add the length of 'subj_' and 3 additional characters to include after 'subj_'\n",
    "    end_pos = start_pos + len('subj_') + 3\n",
    "    # Slice the recording key to get everything up to and including the subject identifier plus three characters\n",
    "    recording_key_without_suffix = recording[:end_pos]\n",
    "    if recording_key_without_suffix in timestamp_dicts:\n",
    "        # Assign the corresponding timestamp_dicts dictionary to event_dict\n",
    "        ephys_data.collection[recording].event_dict = timestamp_dicts[recording_key_without_suffix]\n",
    "        \n",
    "        # Extract the subject from the recording key\n",
    "        start = recording.find('subj_') + 5  # Start index after 'subj_'\n",
    "        subject = recording[start:]\n",
    "        \n",
    "        # Assign the extracted subject\n",
    "        ephys_data.collection[recording].subject = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44831876-9721-452f-a504-92c619623d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These recordings are missing event dictionaries:\n",
      "['20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec']\n",
      "These recordings are missing subjects: ['20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec']\n"
     ]
    }
   ],
   "source": [
    "spike_analysis = spike.SpikeAnalysis_MultiRecording(ephys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd27a766-b819-4b1d-9689-e34214ae3e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20230612_101430_standard_comp_to_training_D1_subj_1-3',\n",
       " '20230612_101430_standard_comp_to_training_D1_subj_1-4',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-1',\n",
       " '20230612_112630_standard_comp_to_training_D1_subj_1-2',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-1',\n",
       " '20230613_105657_standard_comp_to_training_D2_subj_1-4',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-1',\n",
       " '20230614_114041_standard_comp_to_training_D3_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-2',\n",
       " '20230616_111904_standard_comp_to_training_D4_subj_1-4',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-1',\n",
       " '20230617_115521_standard_comp_to_omission_D1_subj_1-2',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-1',\n",
       " '20230618_100636_standard_comp_to_omission_D2_subj_1-4',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-1',\n",
       " '20230620_114347_standard_comp_to_omission_D4_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-2',\n",
       " '20230621_111240_standard_comp_to_omission_D5_subj_1-4',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1',\n",
       " '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2',\n",
       " '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1',\n",
       " '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(timestamp_dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1cb8f-39d8-4a86-868b-14df0af74d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
