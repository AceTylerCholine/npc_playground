{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b471416c-6eab-4dec-86b6-5f775fd8b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import multirecording_spikeanalysis as spike\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe32acfc-7dc3-4530-a810-2d4796adbbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_1_t1b2L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230618_100636_standard_comp_to_omission_D2_subj_1_4_t4b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 92 is unsorted & has 2494 spikes\n",
      "Unit 92 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_t1b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 103 is unsorted & has 512 spikes\n",
      "Unit 103 will be deleted\n",
      "20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_t1b2L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 96 is unsorted & has 5811 spikes\n",
      "Unit 96 will be deleted\n",
      "Unit 95 is unsorted & has 6458 spikes\n",
      "Unit 95 will be deleted\n",
      "20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4_t3b3L_box1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs1-2and2-2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Unit 151 is unsorted & has 8528 spikes\n",
      "Unit 151 will be deleted\n",
      "20230628_111202_standard_comp_to_novel_agent_D1_subj_1-2vs1-1and2-1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1v1-4and2-1_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230629_111937_standard_comp_to_novel_agent_D2_subj_1-4vs1-1and2-2_merged.rec\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4vs1-2and2-1_merged_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "rcecollection = spike.EphysRecordingCollection(r'.\\export\\finished_curation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c252738c-8e84-470c-bb66-7b1d2da6ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftones = pd.read_excel('rce_pilot_2_per_video_trial_labels.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97a8961-7690-4531-9333-2c65287eb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dictionaries = {}\n",
    "\n",
    "for video_name in dftones['video_name'].unique():\n",
    "    # Extract rows for the current video_name\n",
    "    video_rows = dftones[dftones['video_name'] == video_name]\n",
    "    \n",
    "    # Create a NumPy array of len-2 arrays for start and stop timestamps\n",
    "    timestamps_array = np.array(list(zip(video_rows['tone_start_timestamp'], video_rows['tone_stop_timestamp'])))\n",
    "    \n",
    "    # Create a dictionary for each video_name with tone_timestamps as the key\n",
    "    video_dictionaries[video_name] = {'tone_timestamps': timestamps_array}\n",
    "\n",
    "# Identify keys to remove, including both the NaN key and the 2nd video for each recording\n",
    "keys_to_remove = [key for key in video_dictionaries.keys() if (isinstance(key, float) and np.isnan(key)) or (isinstance(key, str) and key.endswith('.2.videoTimeStamps.cameraHWSync'))]\n",
    "\n",
    "# Remove the identified keys\n",
    "for key in keys_to_remove:\n",
    "    del video_dictionaries[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c8bb35-c40a-4bf5-b6d6-5af6905dbe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tone_start_frame</th>\n",
       "      <th>reward_start_frame</th>\n",
       "      <th>tone_stop_frame</th>\n",
       "      <th>box_1_port_entry_frames</th>\n",
       "      <th>box_2_port_entry_frames</th>\n",
       "      <th>condition</th>\n",
       "      <th>competition_closeness</th>\n",
       "      <th>notes</th>\n",
       "      <th>experiment</th>\n",
       "      <th>session_dir</th>\n",
       "      <th>all_subjects</th>\n",
       "      <th>first_timestamp</th>\n",
       "      <th>last_timestamp</th>\n",
       "      <th>tone_start_timestamp</th>\n",
       "      <th>tone_stop_timestamp</th>\n",
       "      <th>box_1_port_entry_timestamps</th>\n",
       "      <th>box_2_port_entry_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>condition column has rewarded if alone; ID of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>980.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>[1028 1031] [1149 1266]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>NaN</td>\n",
       "      <td>standard</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>8798886.0</td>\n",
       "      <td>77093151.0</td>\n",
       "      <td>982229.0</td>\n",
       "      <td>1182226.0</td>\n",
       "      <td>[1030229 1033226] [1151634 1269428]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>3376.0</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>3575.0</td>\n",
       "      <td>[3545 3545] [3547 3549] [3550 3554] [3554 3556...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Subj 1 Only</td>\n",
       "      <td>NaN</td>\n",
       "      <td>standard</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>8798886.0</td>\n",
       "      <td>77093151.0</td>\n",
       "      <td>3382227.0</td>\n",
       "      <td>3582224.0</td>\n",
       "      <td>[3550827 3551624] [3553824 3555829] [3556426 3...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>5672.0</td>\n",
       "      <td>5752.0</td>\n",
       "      <td>5871.0</td>\n",
       "      <td>[5761 5762] [5762 5942]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>NaN</td>\n",
       "      <td>standard</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>8798886.0</td>\n",
       "      <td>77093151.0</td>\n",
       "      <td>5682225.0</td>\n",
       "      <td>5882222.0</td>\n",
       "      <td>[5771223 5772822] [5773422 5952622]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>7468.0</td>\n",
       "      <td>7548.0</td>\n",
       "      <td>7668.0</td>\n",
       "      <td>[7632 7634] [7635 7665] [7665 7945]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>NaN</td>\n",
       "      <td>standard</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_s...</td>\n",
       "      <td>['1.3', '1.4']</td>\n",
       "      <td>8798886.0</td>\n",
       "      <td>77093151.0</td>\n",
       "      <td>7482224.0</td>\n",
       "      <td>7682221.0</td>\n",
       "      <td>[7647221 7648224] [7649024 7679421] [7680023 7...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>58.0</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>60714.0</td>\n",
       "      <td>60794.0</td>\n",
       "      <td>60914.0</td>\n",
       "      <td>[60676 60746] [60777 60803] [60824 60897] [609...</td>\n",
       "      <td>[60819 60821] [60821 61167]</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>2.2 Chase or biter 1.4 then grab after trial i...</td>\n",
       "      <td>novel</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>10971930.0</td>\n",
       "      <td>79536439.0</td>\n",
       "      <td>60673457.0</td>\n",
       "      <td>60873459.0</td>\n",
       "      <td>[60635256 60705256] [60736657 60762662] [60782...</td>\n",
       "      <td>[60779058 60780258] [60780662 61127062]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>58.0</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>62211.0</td>\n",
       "      <td>62291.0</td>\n",
       "      <td>62410.0</td>\n",
       "      <td>[62160 62396]</td>\n",
       "      <td>[62226 62228] [62230 62236] [62260 62283] [622...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>2.2 Chase or biter 1.4 then grab after trial i...</td>\n",
       "      <td>novel</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>10971930.0</td>\n",
       "      <td>79536439.0</td>\n",
       "      <td>62173475.0</td>\n",
       "      <td>62373477.0</td>\n",
       "      <td>[62121874 62359482]</td>\n",
       "      <td>[62189072 62190075] [62193075 62198472] [62221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>58.0</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>64207.0</td>\n",
       "      <td>64287.0</td>\n",
       "      <td>64406.0</td>\n",
       "      <td>[64188 64391] [64393 64738]</td>\n",
       "      <td>[64298 64477]</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>2.2 Chase or biter 1.4 then grab after trial i...</td>\n",
       "      <td>novel</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>10971930.0</td>\n",
       "      <td>79536439.0</td>\n",
       "      <td>64173502.0</td>\n",
       "      <td>64373501.0</td>\n",
       "      <td>[64154499 64357501] [64359306 64705110]</td>\n",
       "      <td>[64264904 64443702]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>58.0</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>66104.0</td>\n",
       "      <td>66184.0</td>\n",
       "      <td>66303.0</td>\n",
       "      <td>[66124 66274] [66275 66276] [66278 66523]</td>\n",
       "      <td>[66153 66163] [66256 66257] [66257 66319]</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>2.2 Chase or biter 1.4 then grab after trial i...</td>\n",
       "      <td>novel</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>10971930.0</td>\n",
       "      <td>79536439.0</td>\n",
       "      <td>66073522.0</td>\n",
       "      <td>66273524.0</td>\n",
       "      <td>[66093722 66244528] [66244928 66246724] [66248...</td>\n",
       "      <td>[66122925 66133522] [66226128 66227328] [66227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>58.0</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>67102.0</td>\n",
       "      <td>67182.0</td>\n",
       "      <td>67301.0</td>\n",
       "      <td>[67115 67213] [67231 67606]</td>\n",
       "      <td>[67116 67144] [67158 67164] [67199 67498]</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Subj 2 Only</td>\n",
       "      <td>2.2 Chase or biter 1.4 then grab after trial i...</td>\n",
       "      <td>novel</td>\n",
       "      <td>20230630_115506_standard_comp_to_novel_agent_D...</td>\n",
       "      <td>['1.2', '1.4', '2.1', '2.2']</td>\n",
       "      <td>10971930.0</td>\n",
       "      <td>79536439.0</td>\n",
       "      <td>67073535.0</td>\n",
       "      <td>67273537.0</td>\n",
       "      <td>[67086534 67184943] [67202335 67578740]</td>\n",
       "      <td>[67087934 67115334] [67129134 67135935] [67170...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1263 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                         video_name  \\\n",
       "0            NaN                                                NaN   \n",
       "1            0.0  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "2            0.0  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "3            0.0  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "4            0.0  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "...          ...                                                ...   \n",
       "1258        58.0  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1259        58.0  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1260        58.0  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1261        58.0  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1262        58.0  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "\n",
       "      tone_start_frame  reward_start_frame  tone_stop_frame  \\\n",
       "0                  NaN                 NaN              NaN   \n",
       "1                980.0              1060.0           1181.0   \n",
       "2               3376.0              3456.0           3575.0   \n",
       "3               5672.0              5752.0           5871.0   \n",
       "4               7468.0              7548.0           7668.0   \n",
       "...                ...                 ...              ...   \n",
       "1258           60714.0             60794.0          60914.0   \n",
       "1259           62211.0             62291.0          62410.0   \n",
       "1260           64207.0             64287.0          64406.0   \n",
       "1261           66104.0             66184.0          66303.0   \n",
       "1262           67102.0             67182.0          67301.0   \n",
       "\n",
       "                                box_1_port_entry_frames  \\\n",
       "0                                                   NaN   \n",
       "1                               [1028 1031] [1149 1266]   \n",
       "2     [3545 3545] [3547 3549] [3550 3554] [3554 3556...   \n",
       "3                               [5761 5762] [5762 5942]   \n",
       "4                   [7632 7634] [7635 7665] [7665 7945]   \n",
       "...                                                 ...   \n",
       "1258  [60676 60746] [60777 60803] [60824 60897] [609...   \n",
       "1259                                      [62160 62396]   \n",
       "1260                        [64188 64391] [64393 64738]   \n",
       "1261          [66124 66274] [66275 66276] [66278 66523]   \n",
       "1262                        [67115 67213] [67231 67606]   \n",
       "\n",
       "                                box_2_port_entry_frames condition   \\\n",
       "0                                                   NaN        NaN   \n",
       "1                                                   NaN        1.4   \n",
       "2                                                   NaN        1.3   \n",
       "3                                                   NaN        1.4   \n",
       "4                                                   NaN        1.4   \n",
       "...                                                 ...        ...   \n",
       "1258                        [60819 60821] [60821 61167]        2.2   \n",
       "1259  [62226 62228] [62230 62236] [62260 62283] [622...        2.2   \n",
       "1260                                      [64298 64477]        2.2   \n",
       "1261          [66153 66163] [66256 66257] [66257 66319]        2.2   \n",
       "1262          [67116 67144] [67158 67164] [67199 67498]        2.2   \n",
       "\n",
       "     competition_closeness                                              notes  \\\n",
       "0                      NaN  condition column has rewarded if alone; ID of ...   \n",
       "1              Subj 2 Only                                                NaN   \n",
       "2              Subj 1 Only                                                NaN   \n",
       "3              Subj 2 Only                                                NaN   \n",
       "4              Subj 2 Only                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "1258           Subj 2 Only  2.2 Chase or biter 1.4 then grab after trial i...   \n",
       "1259           Subj 2 Only  2.2 Chase or biter 1.4 then grab after trial i...   \n",
       "1260           Subj 2 Only  2.2 Chase or biter 1.4 then grab after trial i...   \n",
       "1261           Subj 2 Only  2.2 Chase or biter 1.4 then grab after trial i...   \n",
       "1262           Subj 2 Only  2.2 Chase or biter 1.4 then grab after trial i...   \n",
       "\n",
       "     experiment                                        session_dir  \\\n",
       "0           NaN                                                NaN   \n",
       "1      standard  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "2      standard  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "3      standard  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "4      standard  20230612_101430_standard_comp_to_training_D1_s...   \n",
       "...         ...                                                ...   \n",
       "1258      novel  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1259      novel  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1260      novel  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1261      novel  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "1262      novel  20230630_115506_standard_comp_to_novel_agent_D...   \n",
       "\n",
       "                      all_subjects  first_timestamp  last_timestamp  \\\n",
       "0                              NaN              NaN             NaN   \n",
       "1                   ['1.3', '1.4']        8798886.0      77093151.0   \n",
       "2                   ['1.3', '1.4']        8798886.0      77093151.0   \n",
       "3                   ['1.3', '1.4']        8798886.0      77093151.0   \n",
       "4                   ['1.3', '1.4']        8798886.0      77093151.0   \n",
       "...                            ...              ...             ...   \n",
       "1258  ['1.2', '1.4', '2.1', '2.2']       10971930.0      79536439.0   \n",
       "1259  ['1.2', '1.4', '2.1', '2.2']       10971930.0      79536439.0   \n",
       "1260  ['1.2', '1.4', '2.1', '2.2']       10971930.0      79536439.0   \n",
       "1261  ['1.2', '1.4', '2.1', '2.2']       10971930.0      79536439.0   \n",
       "1262  ['1.2', '1.4', '2.1', '2.2']       10971930.0      79536439.0   \n",
       "\n",
       "      tone_start_timestamp  tone_stop_timestamp  \\\n",
       "0                      NaN                  NaN   \n",
       "1                 982229.0            1182226.0   \n",
       "2                3382227.0            3582224.0   \n",
       "3                5682225.0            5882222.0   \n",
       "4                7482224.0            7682221.0   \n",
       "...                    ...                  ...   \n",
       "1258            60673457.0           60873459.0   \n",
       "1259            62173475.0           62373477.0   \n",
       "1260            64173502.0           64373501.0   \n",
       "1261            66073522.0           66273524.0   \n",
       "1262            67073535.0           67273537.0   \n",
       "\n",
       "                            box_1_port_entry_timestamps  \\\n",
       "0                                                   NaN   \n",
       "1                   [1030229 1033226] [1151634 1269428]   \n",
       "2     [3550827 3551624] [3553824 3555829] [3556426 3...   \n",
       "3                   [5771223 5772822] [5773422 5952622]   \n",
       "4     [7647221 7648224] [7649024 7679421] [7680023 7...   \n",
       "...                                                 ...   \n",
       "1258  [60635256 60705256] [60736657 60762662] [60782...   \n",
       "1259                                [62121874 62359482]   \n",
       "1260            [64154499 64357501] [64359306 64705110]   \n",
       "1261  [66093722 66244528] [66244928 66246724] [66248...   \n",
       "1262            [67086534 67184943] [67202335 67578740]   \n",
       "\n",
       "                            box_2_port_entry_timestamps  \n",
       "0                                                   NaN  \n",
       "1                                                   NaN  \n",
       "2                                                   NaN  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "1258            [60779058 60780258] [60780662 61127062]  \n",
       "1259  [62189072 62190075] [62193075 62198472] [62221...  \n",
       "1260                                [64264904 64443702]  \n",
       "1261  [66122925 66133522] [66226128 66227328] [66227...  \n",
       "1262  [67087934 67115334] [67129134 67135935] [67170...  \n",
       "\n",
       "[1263 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8500c6cb-3c0e-43c0-83f2-1116b1b93ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20230612_101430_standard_comp_to_training_D1_subj_1-4_and_1-3.1.videoTimeStamps.cameraHWSync', '20230612_112630_standard_comp_to_training_D1_subj_1-2_and_1-1.1.videoTimeStamps.cameraHWSync', '20230613_105657_standard_comp_to_training_D2_subj_1-1_and_1-4.1.videoTimeStamps.cameraHWSync', '20230614_114041_standard_comp_to_training_D3_subj_1-1_and_1-2.1.videoTimeStamps.cameraHWSync', '20230614_114041_standard_comp_to_training_D3_subj_1-1_and_1-2.3.videoTimeStamps.cameraHWSync', '20230616_111904_standard_comp_to_training_D4_subj_1-4_and_1-2.1.videoTimeStamps.cameraHWSync', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_and_1-2.1.videoTimeStamps.cameraHWSync', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_and_1-2.3.videoTimeStamps.cameraHWSync', '20230618_100636_standard_comp_to_omission_D2_subj_1-4_and_1-1.1.videoTimeStamps.cameraHWSync', '20230619_115321_standard_comp_to_omission_D3_subj_1-2_and_1-4.4.videoTimeStamps.cameraHWSync', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_and_1-1.1.videoTimeStamps.cameraHWSync', '20230621_111240_standard_comp_to_omission_D5_subj_1-4_and_1-2.1.videoTimeStamps.cameraHWSync', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_and_1-2.1.videoTimeStamps.cameraHWSync', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_and_1-4.1.videoTimeStamps.cameraHWSync', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_and_1-4.1.videoTimeStamps.cameraHWSync', '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs2-2and1-2vs2-1.1.videoTimeStamps.cameraHWSync', '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs2-2and1-2vs2-1.3.videoTimeStamps.cameraHWSync', '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs2-2and1-2vs2-1.4.videoTimeStamps.cameraHWSync', '20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1vs2-1and1-4vs2-2.1.videoTimeStamps.cameraHWSync', '20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4vs2-1and1-2vs2-2.1.videoTimeStamps.cameraHWSync'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dictionaries.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca13041-c527-4e25-9a67-4d76b47124c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying session_dir\n",
    "video_dictionaries2 = {}\n",
    "\n",
    "for session_dir in dftones['session_dir'].unique():\n",
    "    # Extract rows for the current session_dir\n",
    "    session_rows = dftones[dftones['session_dir'] == session_dir]\n",
    "    \n",
    "    # Create a NumPy array of len-2 arrays for start and stop timestamps\n",
    "    timestamps_array = np.array(list(zip(video_rows['tone_start_timestamp'], session_rows['tone_stop_timestamp'])))\n",
    "    \n",
    "    # Create a dictionary for each video_name with tone_timestamps as the key\n",
    "    video_dictionaries2[session_dir] = {'tone_timestamps': timestamps_array}\n",
    "\n",
    "# Identify keys to remove, including both the NaN key and the 2nd video for each recording\n",
    "keys_to_remove = [key for key in video_dictionaries2.keys() if (isinstance(key, float) and np.isnan(key))]\n",
    "\n",
    "# Remove the identified keys\n",
    "for key in keys_to_remove:\n",
    "    del video_dictionaries2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2676ce99-df9c-4293-872e-f33353f81233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20230612_101430_standard_comp_to_training_D1_subj_1-4_and_1-3', '20230612_112630_standard_comp_to_training_D1_subj_1-2_and_1-1', '20230613_105657_standard_comp_to_training_D2_subj_1-1_and_1-4', '20230614_114041_standard_comp_to_training_D3_subj_1-1_and_1-2', '20230616_111904_standard_comp_to_training_D4_subj_1-4_and_1-2', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_and_1-2', '20230618_100636_standard_comp_to_omission_D2_subj_1-4_and_1-1', '20230619_115321_standard_comp_to_omission_D3_subj_1-2_and_1-4', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_and_1-1', '20230621_111240_standard_comp_to_omission_D5_subj_1-4_and_1-2', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1_and_1-2', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2_and_1-4', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1_and_1-4', '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs2-2and1-2vs2-1', '20230629_111937_standard_comp_to_novel_agent_D2_subj_1-1vs2-1and1-4vs2-2', '20230630_115506_standard_comp_to_novel_agent_D3_subj_1-4vs2-1and1-2vs2-2'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dictionaries2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7727e4b-4a0d-4d2a-aaa6-d4e318e64567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f2f8b5-97ce-4cd2-9657-c7c3aec71f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_dictionaries2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30896074-d992-46c6-bfa2-70e0ab277a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys unique to video_dictionaries (after adjustment): {'20230619_115321_standard_comp_to_omission_D3_subj_1-2_and_1-4.4.videoTimeStamps.cameraHWSync', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_and_1-2.3.videoTimeStamps.cameraHWSync', '20230614_114041_standard_comp_to_training_D3_subj_1-1_and_1-2.3.videoTimeStamps.cameraHWSync', '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs2-2and1-2vs2-1.4.videoTimeStamps.cameraHWSync', '20230628_111202_standard_comp_to_novel_agent_D1_subj_1-1vs2-2and1-2vs2-1.3.videoTimeStamps.cameraHWSync'}\n",
      "Keys unique to video_dictionaries2: {'20230619_115321_standard_comp_to_omission_D3_subj_1-2_and_1-4'}\n"
     ]
    }
   ],
   "source": [
    "# Adjust keys in video_dictionaries by removing the specified suffix\n",
    "adjusted_video_dictionaries = {key.replace('.1.videoTimeStamps.cameraHWSync', ''): value \n",
    "                               for key, value in video_dictionaries.items()}\n",
    "\n",
    "# Find differences between the keys of the two dictionaries\n",
    "keys_video_dictionaries = set(adjusted_video_dictionaries.keys())\n",
    "keys_video_dictionaries2 = set(video_dictionaries2.keys())\n",
    "\n",
    "# Identifying keys that are unique to each dictionary\n",
    "unique_to_video_dictionaries = keys_video_dictionaries - keys_video_dictionaries2\n",
    "unique_to_video_dictionaries2 = keys_video_dictionaries2 - keys_video_dictionaries\n",
    "\n",
    "# Print the differences\n",
    "print(\"Keys unique to video_dictionaries (after adjustment):\", unique_to_video_dictionaries)\n",
    "print(\"Keys unique to video_dictionaries2:\", unique_to_video_dictionaries2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1f3ff-6ddf-49ed-b9b4-5461d8984cf1",
   "metadata": {},
   "source": [
    "## Some of the video names end with .3 or .4.videoTime... instead of .1.videoTime...\n",
    "## But does that explain why there are more in the video_name dictionary? Are there duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16f08d43-aa33-4b59-a7a3-eeb45d934cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys unique to adjusted video_dictionaries: set()\n",
      "Keys unique to video_dictionaries2: set()\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "# Define the suffixes to be removed\n",
    "suffixes = ['.1.videoTimeStamps.cameraHWSync', '.3.videoTimeStamps.cameraHWSync', '.4.videoTimeStamps.cameraHWSync']\n",
    "\n",
    "# Adjust keys in video_dictionaries by removing any specified suffixes\n",
    "adjusted_video_dictionaries = {\n",
    "    # Remove suffix if present\n",
    "    reduce(lambda key, suffix: key.replace(suffix, ''), suffixes, key): value \n",
    "    for key, value in video_dictionaries.items()\n",
    "}\n",
    "\n",
    "# Compare keys between the adjusted video_dictionaries and video_dictionaries2\n",
    "keys_video_dictionaries = set(adjusted_video_dictionaries.keys())\n",
    "keys_video_dictionaries2 = set(video_dictionaries2.keys())\n",
    "\n",
    "# Find unique keys in each dictionary\n",
    "unique_to_video_dictionaries = keys_video_dictionaries - keys_video_dictionaries2\n",
    "unique_to_video_dictionaries2 = keys_video_dictionaries2 - keys_video_dictionaries\n",
    "\n",
    "# Print the differences\n",
    "print(\"Keys unique to adjusted video_dictionaries:\", unique_to_video_dictionaries)\n",
    "print(\"Keys unique to video_dictionaries2:\", unique_to_video_dictionaries2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17cdc03c-39b4-438d-9320-2d2408f535a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjusted_video_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2977fa-3d9b-4d73-9fa8-34a7e6b85243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_dictionaries2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f98b3-66ba-4f1d-81c9-39f4538a5166",
   "metadata": {},
   "source": [
    "## So session_dir is better than video_name because video_name has multiple forms of the same video. I think? Does this mean that the session_dir is going to have duplicate entries within itself or does that mean that the full data wasn't in any 1 specific video_name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e2f72-cbf8-4aba-9a3a-66a528e00a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
